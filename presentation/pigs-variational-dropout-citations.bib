@article{hinton_improving_2012,
	title = {Improving neural networks by preventing co-adaptation of feature detectors},
	url = {http://arxiv.org/abs/1207.0580},
	abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
	urldate = {2015-08-15},
	journal = {arXiv:1207.0580 [cs]},
	author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
	month = jul,
	year = {2012},
	note = {arXiv: 1207.0580},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing}
}

@article{srivastava2014dropout,
    title={Dropout: A simple way to prevent neural networks from overfitting},
    author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
    journal={The Journal of Machine Learning Research},
    volume={15},
    number={1},
    pages={1929--1958},
    year={2014},
    publisher={JMLR. org}
}

@article{gregor_draw_2015,
	title = {{DRAW}: {A} {Recurrent} {Neural} {Network} {For} {Image} {Generation}},
	shorttitle = {{DRAW}},
	url = {http://arxiv.org/abs/1502.04623},
	abstract = {This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.},
	urldate = {2015-05-25},
	journal = {arXiv:1502.04623 [cs]},
	author = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.04623},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing}
}

@inproceedings{courvilledlss2015,
    author = "Aaron Courville",
    title = "Variational Autoencoders and Extensions",
    publisher = "Presented at the Deep Learning Summer School, Montreal",
    year = 2015,
    url = {http://videolectures.net/deeplearning2015_courville_autoencoder_extension/}
}

@article{kingma_auto-encoding_2013,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2015-05-25},
	journal = {arXiv:1312.6114 [cs, stat]},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.6114},
	keywords = {Computer Science - Learning, Statistics - Machine Learning}
}

@book{mackay_information_2003,
	title = {Information {Theory}, {Inference}, and {Learning} {Algorithms}},
	isbn = {0-521-64298-1},
	author = {Mackay, David J C},
	year = {2003}
}

@article{kingma_variational_2015,
	title = {Variational {Dropout} and the {Local} {Reparameterization} {Trick}},
	url = {http://arxiv.org/abs/1506.02557},
	urldate = {2015-06-10},
	journal = {arXiv:1506.02557 [cs, stat]},
	author = {Kingma, Diederik P. and Salimans, Tim and Welling, Max},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.02557},
	keywords = {Computer Science - Learning, Statistics - Computation, Statistics - Machine Learning}
}

@article{cohen_feature_2007,
	title = {Feature {Selection} via {Coalitional} {Game} {Theory}},
	volume = {19},
	issn = {0899-7667},
	url = {http://dx.doi.org/10.1162/neco.2007.19.7.1939},
	doi = {10.1162/neco.2007.19.7.1939},
	abstract = {We present and study the contribution-selection algorithm (CSA), a novel algorithm for feature selection. The algorithm is based on the multiperturbation shapley analysis (MSA), a framework that relies on game theory to estimate usefulness. The algorithm iteratively estimates the usefulness of features and selects them accordingly, using either forward selection or backward elimination. It can optimize various performance measures over unseen data such as accuracy, balanced error rate, and area under receiver-operator-characteristic curve. Empirical comparison with several other existing feature selection methods shows that the backward elimination variant of CSA leads to the most accurate classification results on an array of data sets.},
	number = {7},
	urldate = {2015-08-16},
	journal = {Neural Computation},
	author = {Cohen, Shay and Dror, Gideon and Ruppin, Eytan},
	month = may,
	year = {2007},
	pages = {1939--1961}
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	copyright = {© 2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {0028-0836},
	url = {http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	language = {en},
	number = {7553},
	urldate = {2015-08-14},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	keywords = {Computer science, Mathematics and computing},
	pages = {436--444}
}

@phdthesis{neal_bayesian_1995,
	title = {Bayesian learning for neural networks},
	url = {http://www.db.toronto.edu/~radford/ftp/thesis.pdf},
	urldate = {2014-11-21},
	school = {University of Toronto},
	author = {Neal, Radford M.},
	year = {1995}
}

@incollection{graves_practical_2011,
	title = {Practical {Variational} {Inference} for {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks.pdf},
	urldate = {2015-08-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 24},
	publisher = {Curran Associates, Inc.},
	author = {Graves, Alex},
	editor = {Shawe-Taylor, J. and Zemel, R. S. and Bartlett, P. L. and Pereira, F. and Weinberger, K. Q.},
	year = {2011},
	pages = {2348--2356}
}

@article{hernandez-lobato_probabilistic_2015,
	title = {Probabilistic {Backpropagation} for {Scalable} {Learning} of {Bayesian} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1502.05336},
	abstract = {Large multilayer neural networks trained with backpropagation have recently achieved state-of-the-art results in a wide range of problems. However, using backprop for neural net learning still has some disadvantages, e.g., having to tune a large number of hyperparameters to the data, lack of calibrated probabilistic predictions, and a tendency to overfit the training data. In principle, the Bayesian approach to learning neural networks does not have these problems. However, existing Bayesian techniques lack scalability to large dataset and network sizes. In this work we present a novel scalable method for learning Bayesian neural networks, called probabilistic backpropagation (PBP). Similar to classical backpropagation, PBP works by computing a forward propagation of probabilities through the network and then doing a backward computation of gradients. A series of experiments on ten real-world datasets show that PBP is significantly faster than other techniques, while offering competitive predictive abilities. Our experiments also show that PBP provides accurate estimates of the posterior variance on the network weights.},
	urldate = {2015-05-25},
	journal = {arXiv:1502.05336 [stat]},
	author = {Hernández-Lobato, José Miguel and Adams, Ryan P.},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.05336},
	keywords = {Statistics - Machine Learning}
}

@inproceedings{wang_fast_2013,
	title = {Fast dropout training},
	url = {http://machinelearning.wustl.edu/mlpapers/papers/wang13a},
	urldate = {2015-05-26},
	author = {Wang, Sida and Manning, Christopher},
	year = {2013},
	pages = {118--126},
	file = {Full Text PDF:/home/gavin/.mozilla/firefox/i2j16z7y.default/zotero/storage/5J7KGA7S/Wang and Manning - 2013 - Fast dropout training.pdf:application/pdf;Snapshot:/home/gavin/.mozilla/firefox/i2j16z7y.default/zotero/storage/PDIPSC6Q/wang13a.html:text/html}
}

